{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "efna4tbfhojxjlpta4ug",
   "authorId": "272264884048",
   "authorName": "TOM",
   "authorEmail": "tom.meacham@snowflake.com",
   "sessionId": "9646b48b-c04e-4897-9b6e-3b9d8e02b054",
   "lastEditTime": 1742560989873
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b88e052a-0b2f-41dd-a142-2060565bdf32",
   "metadata": {
    "name": "use_case_md",
    "collapsed": false,
    "resultHeight": 488
   },
   "source": "# Use Case: Interactive Chat Experiance with Documents\n\nWe have a asked to create a chat experiance that allows our users to ask questions of our documents to enhance productivity and efficiency, providing a seamless and intuitive way to interact with and extract information from digital files.\n\nSpecifically for the [State of Michigan EMS Protocol Suite](https://www.michigan.gov/mdhhs/inside-mdhhs/legislationpolicy/ems/protocols/michigan-protocols) which contains:\n- **10** documents (PDF)\n- Containing a total of **405 pages** of text\n\n### The Snowflake Features we use to build the solution are:\n- [Snowflake Notebooks](https://docs.snowflake.com/en/user-guide/ui-snowsight/notebooks)\n- [Snowpark for Python](https://docs.snowflake.com/en/developer-guide/snowpark/python/index)\n- [Directory Tables](https://docs.snowflake.com/en/user-guide/data-load-dirtables)\n- [Cortex Search](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-search/cortex-search-overview)\n- [Cortex LLM Functions](https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions)\n- [Streamlit in Snowflake](https://www.snowflake.com/en/data-cloud/overview/streamlit-in-snowflake/)"
  },
  {
   "cell_type": "markdown",
   "id": "fe4eea7f-1540-49d5-a11f-02718b8063d3",
   "metadata": {
    "name": "prerequisites_md",
    "collapsed": false,
    "resultHeight": 1252
   },
   "source": "# Prerequisites :wrench:\n## :blue[Step 1: Add Required Python Packages]:snake:\n#### :red[*Skip this step if you imported this notebook from Git. The `environment.yml` file handles package installation.]\nAdd the following Python packages by clicking `Packages` button in the upper right of the notebook, and searching for them by name.\n\n```\nsnowflake\n```\n```\nsnowflake-ml-python\n```\n\n***\n\n>> #### :bulb: Did you know?\n>> Snowflake features a native integration with [Anaconda](https://www.anaconda.com/) for package and dependency management out of the box. This includes a curated [Anaconda Channel](https://repo.anaconda.com/pkgs/snowflake/) specifically for Snowflake with thousands of the most popular Python packages. There is no additional cost for such use of the Anaconda packages.\n---\n\n## :blue[Step 2: Load PDF Documents to the Notebook] :page_with_curl:\n\n## How to add files to a Notebook from local computer\n1. In the **Files** tab, to the left of your notebook, we can use the `+` icon to select files to upload.\n1. **Before** adding the documeents:\n    - Locate the field at the bottom of the upload interface named __'Specify the path to an existing folder or create a new one'__ \n    - Type in `MI_EMS_PROTO`\n    - This is the folder name we will load our PDFs into.\n1. Click **Browse** and select the PDF files or drag-and-drop them into the dialog.\n1. Finally, click **Upload** to upload your files.\n\n***\n>> #### :bulb: Did you know?\n>> - Every [Notebook](https://docs.snowflake.com/en/user-guide/ui-snowsight/notebooks) in Snowflake has an internal filesystem for working with files that is only avaiable to the notebook. \n>>      - The files are stored in an internal stage which represents your notebook environment. \n>>      - Files stored in this internal stage persist between sessions.\n>> - Your files can be viewed in the `Files` pane on the **left side of the notebook**.\n---\n"
  },
  {
   "cell_type": "markdown",
   "id": "abe6e32a-b16d-44cd-9ba6-3eb0d236457b",
   "metadata": {
    "name": "snowflake_env_md",
    "collapsed": false,
    "resultHeight": 273
   },
   "source": "# Setup Snowflake Eviroment :female-technologist:\n## :blue[Step 1: Start the Notebook Session] :large_green_circle:\nWith the prerequisites completed, we are ready to start building the solution. We will start the notebook and import the required packages.\n\n1. Click the `Start` button in the upper right of this notebook to start the notebook session.\n1. Import packages and functions that we need for our solution."
  },
  {
   "cell_type": "code",
   "id": "b351ce08-9509-4b7d-9626-6fdd8adbcd87",
   "metadata": {
    "language": "python",
    "name": "imports_py",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 127
   },
   "outputs": [],
   "source": "# import packages\nfrom snowflake.core import Root\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.cortex import complete, extract_answer, summarize, translate\n\n# get Snowflake session\nsession = get_active_session()\n# The entry point of the Snowflake Core Python APIs that manage the Snowflake objects.\nroot = Root(session)\n\n# Show the current session conetext\n# By default the session context will have the database and schema where the Notebook is saved \n# and the warehouse your notebook is running on.\nprint(\"Current Role: \", session.get_current_role())\nprint(\"Current Warehouse: \", session.get_current_warehouse())\nprint(\"Current Database: \", session.get_current_database())\nprint(\"Current Schema: \", session.get_current_schema())\nprint(\"Current Fully Qualified Schema: \", session.get_fully_qualified_current_schema())",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "01dcfc73-6d23-40ff-8ec2-b694adb57e84",
   "metadata": {
    "name": "setup_env_md",
    "collapsed": false,
    "resultHeight": 337
   },
   "source": "## :blue[Step 2: Create resources (or use an *existing* resources) for this solution] :open_file_folder: :gear:\nThe objects we will be creating require a schema to be stored in. We also need a virtual warehouse for the solution to use.\n\n### In the next cells we will...\n1. Assign the names of the following resources to Python variables\n    * Database\n    * Schema\n    * Virtual Warehouse\n1. Use **SQL** to update the session context by referencing our **Python** variables.\n    * If a resource name/type doesn't exist, the SQL code will attempt to create them."
  },
  {
   "cell_type": "code",
   "id": "a27a7832-8bae-47c8-9143-5a9adc322558",
   "metadata": {
    "language": "python",
    "name": "define_env_vars_py",
    "collapsed": false,
    "resultHeight": 127
   },
   "outputs": [],
   "source": "# define variables in Python\n# variables will be accesseable thoughout the rest of the notebook - even in SQL cells! \nsolution_database = 'demo_ems_rag_app'\nsolution_schema = 'app'\nquery_warehouse = 'demo_ems_rag_app_wh'\n\n# helper variable\nfully_qualified_solution_schema = f\"{solution_database}.{solution_schema}\"\n\n# display Python variable definitions\nprint(\"Variable Values:\")\nprint(f\"\"\"solution_database = {solution_database}\"\"\")\nprint(f\"\"\"solution_schema = {solution_schema}\"\"\")\nprint(f\"\"\"query_warehouse = {query_warehouse}\"\"\")\nprint(f\"\"\"fully_qualified_solution_schema = {fully_qualified_solution_schema}\"\"\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e2c7836b-a707-4d86-8204-d74e6f09e4d7",
   "metadata": {
    "language": "sql",
    "name": "setup_env_sql",
    "collapsed": false,
    "resultHeight": 87
   },
   "outputs": [],
   "source": "-- create enviroment resources if they do not already exist\nCREATE DATABASE IF NOT EXISTS IDENTIFIER('{{solution_database}}');\nCREATE SCHEMA IF NOT EXISTS IDENTIFIER('{{fully_qualified_solution_schema}}');\n\nCREATE WAREHOUSE IF NOT EXISTS IDENTIFIER('{{query_warehouse}}') WITH\n     WAREHOUSE_SIZE='X-SMALL'\n     AUTO_SUSPEND = 120\n     AUTO_RESUME = TRUE\n     INITIALLY_SUSPENDED=TRUE;\n\n-- set session context\nUSE WAREHOUSE IDENTIFIER('{{query_warehouse}}');\nUSE SCHEMA IDENTIFIER('{{fully_qualified_solution_schema}}');",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1a1dec28-694c-4a0a-b756-76636db87dcd",
   "metadata": {
    "name": "create_info_md",
    "collapsed": false,
    "resultHeight": 279
   },
   "source": ">> #### :bulb: Did you know?\n>> - The `CREATE DATABASE` statement creates a database. \n>>      - All Snowflake databases automatically includes a schema named `PUBLIC`.\n>> - The `CREATE WAREHOUSE` statement creates an initially suspended warehouse.\n>> - When a new database, schema, or virtual warehouse is created, the session context will implicitly update to the newly created resource.\n>>      - This implicit behavior is unique to the `CREATE` statements.\n>> - You can explicitly update the session context with `USE` statements. "
  },
  {
   "cell_type": "markdown",
   "id": "a5b79732-bfdd-4101-aaba-e2e3411f5215",
   "metadata": {
    "name": "validate_session_context_md",
    "collapsed": false,
    "resultHeight": 159
   },
   "source": "## :blue[Step 3: Validate the Session Context has been updated] :white_check_mark:\n- When we run the cell below, we should see the session context has been updated to the database, schema, and warehouse above.\n- Last time we used Python to check this. This time we will use SQL to check the session context."
  },
  {
   "cell_type": "code",
   "id": "c51b36f0-dde4-49f5-8a83-8ec925fcb78d",
   "metadata": {
    "language": "sql",
    "name": "validate_env_sql",
    "collapsed": false,
    "resultHeight": 216
   },
   "outputs": [],
   "source": "WITH session_details as (\nSELECT 'Warehouse' as current_item, current_warehouse() as actual, upper('{{query_warehouse}}') as expected UNION\nSELECT 'Database', current_database(), upper('{{solution_database}}') UNION\nSELECT 'Schema', current_schema(), upper('{{solution_schema}}') UNION\nSELECT 'Fully Qualified Schema', current_database() || '.' || current_schema(), upper('{{fully_qualified_solution_schema}}')\n)\nSELECT *, iff(actual = expected, '✅ - Pass', '❌ - Fail - Run the 3 cells above and try again.') as validated FROM session_details",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4be17b38-889a-46ea-8acd-e9729bbbe375",
   "metadata": {
    "name": "make_docs_avail_md",
    "collapsed": false,
    "resultHeight": 172
   },
   "source": "## :blue[Step 4: Make the documents available outside of this notebook] :file_cabinet:\nIn order to build the solution, we need to make our documents accessible outside of this notebook. \nTo do this, we will:\n1. Create a permanent named stage in Snowflake to store the files.\n1. Copy the files from the Notebook filesystem to that stage. \n"
  },
  {
   "cell_type": "code",
   "id": "0fb21598-ab17-46cc-8d9e-079d08cf919a",
   "metadata": {
    "language": "sql",
    "name": "create_stage_sql",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "-- variable for name of Snowflake stage\nSET v_stage_name = '{{fully_qualified_solution_schema}}.'||'docs';\n\n-- create stage if not exists\nCREATE STAGE IF NOT EXISTS IDENTIFIER($v_stage_name)\n    DIRECTORY = (ENABLE = TRUE)\n    ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE');",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "98d41c8f-9c44-47a8-919a-6aeb44b4501d",
   "metadata": {
    "name": "get_folder_path_md",
    "collapsed": false,
    "resultHeight": 120
   },
   "source": "### Save the Notebook filesystem path to the document folder to a variable\n- Here we define a Python function to search though the Notebook filesystem to find the path to document folder.\n- Then we set that path to a variable."
  },
  {
   "cell_type": "code",
   "id": "69c9bd5d-4822-42e1-85bc-e04a3a716e0b",
   "metadata": {
    "language": "python",
    "name": "define_folder_path_function_py",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "import os\nimport streamlit as st\n\n# Define python function to search though the file system for the path to a matching directory\ndef list_matching_directories_recursively(path, match_name):\n    \"\"\"\n    Recursively list all directories that match a specified directory name.\n    \n    Args:\n    path (str): The root directory to start the search from.\n    match_name (str): The name of the directory to match.\n    \n    Returns:\n    list: A list of matching directory paths.\n    \"\"\"\n    matching_directories = []  # Initialize an empty list to store matching directories\n    \n    # Walk through the directory tree\n    for root, dirs, files in os.walk(path):\n        # Iterate over the directories in the current root\n        for dir in dirs:\n            # Check if the directory name matches the specified name\n            if dir == match_name:\n                # If it matches, add the full path of the directory to the list\n                matching_directories.append(os.path.join(root, dir))\n    \n    return matching_directories",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a2f97792-b303-4a50-9887-5bb6e6e52f70",
   "metadata": {
    "language": "python",
    "name": "get_folder_path_py",
    "collapsed": false,
    "resultHeight": 529
   },
   "outputs": [],
   "source": "# Specify the path you want to start from and the directory name to match\ndocument_dir = 'MI_EMS_PROTO'\npath = os.getcwd()\nmatch_name = document_dir\n\n# Call the function to get a list of matching directories\nmatching_directories = list_matching_directories_recursively(path, match_name)\n\n# return the path to the first matching directory (assumption is there is only one)\ndocument_path = matching_directories[0]\nst.info(f\"\"\"The Notebook filesystem path to the documents folder is:\n\n**`{document_path}`**\"\"\")\nst.info(f\"Files available in the **`{document_dir}`** folder:\")\nos.listdir(document_path)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a65bd2ff-092a-4111-81c3-fcb1f35d9e38",
   "metadata": {
    "name": "put_files_in_stage_py",
    "collapsed": false,
    "resultHeight": 91
   },
   "source": "### Now we will use Python to save the documents to the named stage and print the results\n- Note: This could be done via the Snowsight GUI, SnowSQL, SnowCLI, and a number of other methods as well"
  },
  {
   "cell_type": "code",
   "id": "115a9098-a78e-4ea0-9d89-076e6a7923a7",
   "metadata": {
    "language": "python",
    "name": "load_files_to_stage_py",
    "collapsed": false,
    "resultHeight": 410
   },
   "outputs": [],
   "source": "import pandas as pd\n\n# Define the stage location \nstage = \"@DOCS\"\n\n# Define the files you want to upload to a stage\nfile = \"*\" # '*' is a wildcard for all files in the directory \n\n# Define the path\nstage_location = \"/\"+os.path.split(document_path)[-1]\n\n# Push the files to the stage with a PUT operation\nput_result = session.file.put(f\"{document_path}/{file}\", \n                              f\"{stage}{stage_location}\", \n                                auto_compress=False, \n                                overwrite=True)\n\n# Convert the result to a pandas DataFrame for readability\ndf = pd.DataFrame(put_result)\n\n# Print the results array\nst.write(df)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d2b452fc-c870-492c-89f1-9a7552e44369",
   "metadata": {
    "name": "directory_table_md",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": "### Validate our files have been saved the stage by querying the Directory Table"
  },
  {
   "cell_type": "code",
   "id": "fd0f2c8e-1c14-420d-841a-1a8d7d5b04c6",
   "metadata": {
    "language": "sql",
    "name": "directory_table_sql",
    "collapsed": false,
    "resultHeight": 426
   },
   "outputs": [],
   "source": "-- Register new files in the stage with the directory table\nALTER STAGE IDENTIFIER($v_stage_name) REFRESH;\n\n-- add the @ character required by the directory table\nSET formatted_stage_name = '@'||$v_stage_name;\n\n-- query the directory table\nSELECT * FROM DIRECTORY($formatted_stage_name);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "30f4fba1-b1e9-44cb-8e2e-ad67091130d5",
   "metadata": {
    "name": "build_solution_md",
    "collapsed": false,
    "resultHeight": 1005
   },
   "source": "# Build the Solution!\n## :blue[Step 1: Process our unstructured documents for Cortex Search and the Chat Application] :hammer:\n\nIn order to make our documents searchable and usable by an LLM we need to:\n1. Extract the text from our PDFs.\n1. Split the text of the document up into smaller blocks of text. This is proccess is called **'chunking'**.\n    - Note: There are [several strategies](https://medium.com/@anuragmishra_27746/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d) for chunking text for RAG, we will be using [RecursiveCharacterTextSplitter](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/) from [LangChain](https://www.langchain.com/), a popular python libary for this task.\n\n\n### In order to accomplish the above, we will next register a Python function in Snowflake. \n- This can make the function availble for other Snowflake users \n- The function can be called like any other function n SQL or Python\n\nWe will do this in SQL, however you can also register a Python function with Snowpark as well.\n\n---\n>> #### :thinking_face: Why bother breaking up the text in the first place?\n>> Chunking text for Retrieval-Augmented Generation (RAG) improves:\n>> \n>> - **Efficiency**: Faster indexing and retrieval.\n>> - **Accuracy**: Better query-to-text matching.\n>> - **Context Management**: Focused and relevant segments.\n>> - **Memory Management**: Handles large texts within memory/token limits.\n>> - **Parallel Processing**: Speeds up processing.\n>> - **Reduced Redundancy**: Minimizes repetitive information.\n---\n\nTo make the function, we will use the following popular Python packages:\n- [langchain](https://pypi.org/project/langchain/) :parrot: :link:\n- [PyPDF2](https://pypi.org/project/PyPDF2/) :page_facing_up:\n- [snowflake-snowpark-python](https://pypi.org/project/snowflake-snowpark-python/) :snowflake:\n\n\n"
  },
  {
   "cell_type": "code",
   "id": "a8ef646c-1234-4d0d-82db-cb7896d9fc31",
   "metadata": {
    "language": "sql",
    "name": "create_chunking_udtf_sql",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "CREATE OR REPLACE FUNCTION pdf_text_chunker(file_url STRING, chunk_size INT, chunk_overlap INT)\nRETURNS TABLE (chunk VARCHAR, page_number INT)\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.11'\nHANDLER = 'pdf_text_chunker'\nPACKAGES = ('snowflake-snowpark-python', 'PyPDF2', 'langchain')\nAS\n$$\n\"\"\"\nThis function reads a PDF file from a specified URL, extracts the text from each page,\nand splits the text into chunks of a given size with a specified overlap. The function \nreturns a table containing the text chunks and their corresponding page numbers.\n\nParameters:\n    file_url (STRING): The URL of the PDF file to be processed.\n    chunk_size (INT): The size of each text chunk.\n    chunk_overlap (INT): The number of characters that overlap between consecutive chunks.\n\nReturns:\n    TABLE: A table with two columns:\n        - chunk (VARCHAR): A chunk of text extracted from the PDF.\n        - page_number (INT): The page number of the PDF from which the chunk was extracted.\n\"\"\"\nfrom snowflake.snowpark.types import StringType, StructField, StructType\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom snowflake.snowpark.files import SnowflakeFile\nimport PyPDF2, io\nimport logging\nimport pandas as pd\n\n# Define the main class for PDF text chunking\nclass pdf_text_chunker:\n\n    # Function to read and extract text from a PDF file\n    def read_pdf(self, file_url: str):\n        logger = logging.getLogger(\"udf_logger\")\n        logger.info(f\"Opening file {file_url}\")\n\n        # Open the PDF file from the provided URL\n        with SnowflakeFile.open(file_url, 'rb') as f:\n            buffer = io.BytesIO(f.readall())\n\n        # Initialize the PDF reader\n        reader = PyPDF2.PdfReader(buffer)\n        texts_and_pages = []\n\n        # Extract text from each page of the PDF\n        for page_number, page in enumerate(reader.pages, start=1):\n            try:\n                text = page.extract_text().replace('\\n', ' ') #.replace('\\0', ' ')\n                texts_and_pages.append((text, page_number))\n            except:\n                text = \"Unable to Extract\"\n                logger.warn(f\"Unable to extract from file {file_url}, page {page_number}\")\n                texts_and_pages.append((text, page_number))\n\n        return texts_and_pages\n\n    # Function to process the PDF and split text into chunks\n    def process(self, file_url: str, chunk_size: int, chunk_overlap: int):\n        # Read text and page numbers from the PDF file\n        texts_and_pages = self.read_pdf(file_url)\n\n        # Initialize the text splitter with given chunk size and overlap\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size = chunk_size,  # Set the chunk size\n            chunk_overlap = chunk_overlap,  # Set the chunk overlap\n            length_function = len\n        )\n\n        chunks_with_pages = []\n        \n        # Split the text into chunks and associate with page numbers\n        for text, page_number in texts_and_pages:\n            chunks = text_splitter.split_text(text)\n            for chunk in chunks:\n                chunks_with_pages.append((chunk, page_number))\n        \n        # Create a DataFrame from the chunks and page numbers\n        df = pd.DataFrame(chunks_with_pages, columns=['chunk', 'page_number'])\n\n        # Yield chunks as tuples\n        yield from df.itertuples(index=False, name=None)\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9759a59a-9061-4a31-9e8e-a269ebab3a6d",
   "metadata": {
    "name": "create_chunks_table_md",
    "collapsed": false,
    "resultHeight": 91
   },
   "source": "### Next, we create a table that maps the documents to the text chunks\n- This table will also have other useful metadata such as the page number the text chunk came from"
  },
  {
   "cell_type": "code",
   "id": "f32d22e8-8533-4e99-982d-582ca63bd0d1",
   "metadata": {
    "language": "sql",
    "name": "create_chunks_table_sql",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE docs_chunks_table \nCHANGE_TRACKING = TRUE\nAS\n    SELECT\n        relative_path,\n        '@DOCS/' || relative_path as relative_path_with_stage,\n        split_part(relative_path, '/', -1) as file_name,\n        replace(relative_path, file_name, '') as folder_path,\n        -- preserve file title and page number by concatenating with the chunk\n        CONCAT(object_construct('document_title', file_name, 'page', page_number)::string, ' : ', func.chunk) AS chunk,\n        func.page_number\n    FROM\n        directory(@docs),\n        TABLE(pdf_text_chunker(\n                    file_url => build_scoped_file_url(@docs, relative_path),\n                    chunk_size => 2000,\n                    chunk_overlap => 300)\n              ) AS func\n    WHERE true\n        AND folder_path = 'MI_EMS_PROTO/';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0675c98d-2587-4027-aaf8-18de5c1637ae",
   "metadata": {
    "name": "explore_chunk_table_md",
    "collapsed": false,
    "resultHeight": 60
   },
   "source": "## Preview and explore the newly created table"
  },
  {
   "cell_type": "code",
   "id": "83d9914a-293c-4923-96c3-ddc5673363eb",
   "metadata": {
    "language": "sql",
    "name": "explore_chunks_table_sql",
    "collapsed": false,
    "resultHeight": 426
   },
   "outputs": [],
   "source": "select \n    file_name,\n    page_number,\n    chunk,\n    --snowflake.cortex.count_tokens('snowflake-arctic-embed-m', chunk) token_count,\n    relative_path,\n    folder_path,\n    relative_path_with_stage\nfrom docs_chunks_table \norder by file_name, page_number\nlimit 10",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dd8f5faf-86b7-4c65-9718-82eb28149763",
   "metadata": {
    "name": "compare_pages_vs_chunks_md",
    "collapsed": false,
    "resultHeight": 60
   },
   "source": "## Compare the number of pages per document to the number of chunks"
  },
  {
   "cell_type": "code",
   "id": "a7d3622d-30b9-49fd-b868-9e958dc3009d",
   "metadata": {
    "language": "sql",
    "name": "compare_pages_vs_chunks_sql",
    "collapsed": false,
    "resultHeight": 426
   },
   "outputs": [],
   "source": "select \n    file_name,\n    max(page_number) as pages,\n    count(*) as chunk_count,\n    chunk_count - pages as chunk_vs_page_count_difference\nfrom docs_chunks_table\ngroup by all\norder by pages desc;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a7d746ca-bbe5-4281-b33c-54cab3dd0d16",
   "metadata": {
    "name": "create_search_srv_md",
    "collapsed": false,
    "resultHeight": 415
   },
   "source": "## :blue[Step 2: Create the Cortex Search Service] :mag:\n\n[Cortex Search](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-search/cortex-search-overview) enables low-latency, high-quality “fuzzy” search over your Snowflake data. Cortex Search powers a broad array of search experiences for Snowflake users including Retrieval Augmented Generation (RAG) applications leveraging Large Language Models (LLMs).\n\nCortex Search gets you up and running with a hybrid (vector and keyword) search engine on your text data in minutes, without having to worry about:\n- Embedding\n- Infrastructure maintenance\n- Search quality parameter tuning\n- Ongoing index refreshes. \n\nThis means you can spend less time on infrastructure and search quality tuning, and more time developing high-quality chat and search experiences using your data.\n"
  },
  {
   "cell_type": "code",
   "id": "7b60defa-df0f-47f9-a1e6-23f90ba6b715",
   "metadata": {
    "language": "sql",
    "name": "create_search_service_sql",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "DROP CORTEX SEARCH SERVICE IF EXISTS EMS_PROTO;\nCREATE OR REPLACE CORTEX SEARCH SERVICE EMS_PROTO\n    ON chunk\n    ATTRIBUTES file_name, page_number\n    WAREHOUSE = {{query_warehouse}}\n    TARGET_LAG = '24 hour'\n    AS (\n    SELECT\n        chunk,\n        file_name,\n        page_number,\n        relative_path,\n        relative_path_with_stage\n    FROM docs_chunks_table \n    );",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a783873d-7859-4410-9b73-ac9134136cf5",
   "metadata": {
    "language": "sql",
    "name": "describe_search_service_sql",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "DESCRIBE CORTEX SEARCH SERVICE EMS_PROTO;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d3216fd7-7132-4ff8-8ea3-be8f2ad2ab3c",
   "metadata": {
    "name": "query_search_service_md",
    "collapsed": false,
    "resultHeight": 169
   },
   "source": "## :blue[Step 3: Query the Cortex Search Service] :magic_wand:\n\nCortex Search offers both a Python and REST API, as well as the ability to preview results with SQL. We will be using the Python interface.\n\nLet's explore what can be done with the the search results..."
  },
  {
   "cell_type": "code",
   "id": "5bfe088c-76e7-4bd2-b453-42f56cbc1e8f",
   "metadata": {
    "language": "python",
    "name": "query_search_service_py",
    "collapsed": false,
    "resultHeight": 1027
   },
   "outputs": [],
   "source": "import streamlit as st\nimport pandas as pd\n\nuser_search = \"What phone number should I call to report suspected adult neglect?\"\n\n# define the search service\nmi_ems_proto_search_service = (root\n  .databases[solution_database]\n  .schemas[solution_schema]\n  .cortex_search_services[\"EMS_PROTO\"]\n)\n\n# first query\n# resp = mi_ems_proto_search_service.search(\n#   query=user_search,\n#   columns=[\"chunk\", \"file_name\", \"page_number\",\"relative_path\",\"relative_path_with_stage\"],\n#   #filter={\"@eq\": {\"region\": \"North America\"} },\n#   limit=1\n# )\n\nresp = mi_ems_proto_search_service.search(\n  query=user_search,\n  columns=[],\n  #filter={\"@eq\": {\"region\": \"North America\"} },\n  limit=1\n)\n\n# second query\nresp2 = mi_ems_proto_search_service.search(\n  query=user_search,\n  columns=[\"file_name\", \"page_number\",\"relative_path_with_stage\"],\n  #filter={\"@eq\": {\"region\": \"North America\"} },\n  limit=5\n)\n\n# display results\nst.header(\"\"\"'Chunk' of text from the documents that most closely matches the question:\"\"\")\nst.markdown(f\"\"\"### :blue[_**'{user_search}'**_]\"\"\")\nsearch_response = resp.to_json()\nst.json(search_response)\n\nst.header(\"Enterprise Document Search\")\nst.subheader(\"Alternatively, we can use the seach service to simply return a listing of files that most closely match search terms.\")\nsearch_response2 = pd.DataFrame(resp2.to_dict()['results'])\nst.write(search_response2)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3618dc3b-e55e-4ec0-8d5b-4cdb6ec067a8",
   "metadata": {
    "name": "pass_results_into_cortex_functions_py",
    "collapsed": false,
    "resultHeight": 60
   },
   "source": "## Example: Passing Cortex Search results though addational Cortex LLM Functions"
  },
  {
   "cell_type": "code",
   "id": "6aa5d8ac-03f2-4607-bfcc-31d9364bba38",
   "metadata": {
    "language": "python",
    "name": "cortex_llm_functions_py",
    "collapsed": false,
    "resultHeight": 832
   },
   "outputs": [],
   "source": "# get responses\nextracted_answer = extract_answer(search_response, user_search)\nsummarized_answer = summarize(search_response)\nsummary_in_spanish = translate(summarized_answer, 'en','es')\n\n# display results\nst.header(\"Built-in Snowflake Cortex LLM Function Examples\")\nst.markdown(\"---\")\n\nst.markdown(\"### [SNOWFLAKE.CORTEX.EXTRACT_ANSWER()](https://docs.snowflake.com/en/sql-reference/functions/extract_answer-snowflake-cortex)\")\nst.write(f\"\"\"**Result for the question: :green['{user_search}']**\"\"\")\nst.json(extracted_answer)\nst.markdown(\"---\")\n\nst.markdown(\"### [SNOWFLAKE.CORTEX.SUMMARIZE()](https://docs.snowflake.com/en/sql-reference/functions/summarize-snowflake-cortex)\")\nst.write(summarized_answer)\nst.markdown(\"---\")\n\nst.markdown(\"### [SNOWFLAKE.CORTEX.TRANSLATE()](https://docs.snowflake.com/en/sql-reference/functions/translate-snowflake-cortex)\")\nst.write(\"**(Translation of the above summary into Spanish)**\")\nst.write(summary_in_spanish)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "250cc03e-9c73-4150-b696-26c43fb2dc3e",
   "metadata": {
    "name": "cortex_complete_example_md",
    "collapsed": false,
    "resultHeight": 371
   },
   "source": "## [SNOWFLAKE.CORTEX.COMPLETE()](https://docs.snowflake.com/en/sql-reference/functions/complete-snowflake-cortex)\nGiven a prompt, generates a response (completion) using your choice of supported language model.\n\n`SNOWFLAKE.CORTEX.COMPLETE()` is the primary Cortex function that will enable our users to chat with our documents. \n\nWith this function you can:\n- Choose which Snowflake-hosted Large Language Model (LLM) to use.\n- Provide the model instructions and context for the type of response you want.\n\n**SQL Syntax**\n```sql\nSELECT SNOWFLAKE.CORTEX.COMPLETE(<model>, <prompt_or_history> [ , <options> ] ) AS RESPONSE;\n```\n\n"
  },
  {
   "cell_type": "code",
   "id": "a34120c2-3f5f-425f-adac-5e67f2b27072",
   "metadata": {
    "language": "python",
    "name": "cortext_complete_py",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 87
   },
   "outputs": [],
   "source": "model = 'snowflake-llama-3.3-70b'\nuser_question = 'What phone number should I call to report suspected adult neglect?'\nprompt_context = search_response\n\nprompt = f\"\"\"\n        [INST]\n        You are a helpful AI chat assistant with RAG capabilities. When a user asks you a question,\n        you will also be given context provided between <context> and </context> tags. \n        Ensure the answer is coherent, concise, and directly relevant to the user's question.\n\n        If the user asks a generic question which cannot be answered with the given context or chat_history,\n        just say \"I don't know the answer to that question.\n        \n        Don't say phases like \"according to the provided information\".\n        \n        <context>          \n        {prompt_context}\n        </context>\n        <question>  \n        {user_question}\n        </question>\n        [/INST]\n        Answer:\n       \"\"\"\ncortex_reponse = complete(model, prompt)\n\nst.header('Example: Simulated Chat Experience with CORTEX.COMPLETE()')\n\nst.markdown(\"---\")\n\nwith st.chat_message(\"ai\"):\n    st.write(\"👋 Hi, I am an AI assistent that can answer questions related to the Michigan EMS Protocal Suite. How can I help you today?\")\nwith st.chat_message(\"user\"):\n    st.write(user_question)\nwith st.chat_message(\"ai\"):\n    st.write(cortex_reponse)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "04f99d1e-202c-4a5f-97ca-272160281622",
   "metadata": {
    "name": "make_streamlit_md",
    "collapsed": false,
    "resultHeight": 143
   },
   "source": "## :blue[Step 4: Deploy an Interactive Chat Experiance for Users with Streamlit in Snowflake] :balloon:\n\nWith all the foundational pieces in place, we can now securely deploy our Chat Application directly in Snowflake with Streamlit.\n\nWe can deploy the Streamlit in Snowflake app directly from this notebook!\n"
  },
  {
   "cell_type": "markdown",
   "id": "028bd3b8-6f7e-4104-8082-16d89e096924",
   "metadata": {
    "name": "create_streamlit_stage_md",
    "collapsed": false,
    "resultHeight": 155
   },
   "source": "#### First, let's create a stage to hold our Streamlit Application files.\n** Note: This will no longer be needed once Multi-page editing is available in Streamlit in Snowflake. At that time, all app files will be contained directly in the Streamlit Object for newly created apps.  \n- [Work with files in Streamlit in Snowflake](https://docs.snowflake.com/en/LIMITEDACCESS/streamlit/work-with-files)"
  },
  {
   "cell_type": "code",
   "id": "db562a83-3fb7-4940-82b0-4083330b3558",
   "metadata": {
    "language": "sql",
    "name": "create_streamlit_stage_sql",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "-- variable for name of Snowflake stage\nSET st_stage_name = '{{fully_qualified_solution_schema}}.'||'mi_ems_proto_app';\n\n-- create stage if not exists\nCREATE OR REPLACE STAGE IDENTIFIER($st_stage_name)\n    DIRECTORY = (ENABLE = TRUE)\n    COMMENT = 'Stage for Streamlit App files';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e7347ee8-d7c3-4ba2-ac10-ee0546cf2405",
   "metadata": {
    "name": "load_streamlit_files_md",
    "collapsed": false,
    "resultHeight": 47
   },
   "source": "#### Now we will load the streamlit app files to the stage we just created."
  },
  {
   "cell_type": "code",
   "id": "13802399-26f3-4aa6-9cd0-232d45e4f1a6",
   "metadata": {
    "language": "python",
    "name": "load_streamlit_files_py",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 535
   },
   "outputs": [],
   "source": "# Specify the path you want to start from and the directory name to match\ndocument_dir = 'streamlit_app'\npath = os.getcwd()\nmatch_name = document_dir\n\n# Call the function to get a list of matching directories\nmatching_directories = list_matching_directories_recursively(path, match_name)\n\n# return the path to the first matching directory (assumption is there is only one)\nst_files_path = matching_directories[0]\nst.info(f\"\"\"The Notebook filesystem path to the streamlit files folder is:\n\n**`{st_files_path}`**\"\"\")\nst.info(f\"Files available in the **`{document_dir}`** folder:\")\nst.write(os.listdir(st_files_path))\n\n\n# Define the stage location \nst_stage = f\"\"\"@{fully_qualified_solution_schema}.streamlit\"\"\"\n\n# Define the files you want to upload to a stage\nst_file = \"*\" # '*' is a wildcard for all files in the directory \n\n# Define the path\nst_stage_location = \"/\"+os.path.split(st_files_path)[-1]\n\n# executing\nprint(\"Executing...\")\nprint(f\"\"\"session.file.put({st_files_path}/{st_file}, {st_stage}, auto_compress=False, overwrite=True)\"\"\")\n\n# Push the files to the stage with a PUT operation\nput_result = session.file.put(f\"{st_files_path}/{st_file}\", \n                              f\"{st_stage}\", \n                                auto_compress=False, \n                                overwrite=True)\n\n# Convert the result to a pandas DataFrame for readability\ndf = pd.DataFrame(put_result)\n\n# Print the results\nst.write(\"**Streamlit Files Uploaded:**\")\nst.write(df)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "16a95d0e-91ec-4e3e-98e9-f76ceba9cca2",
   "metadata": {
    "name": "deploy_streamlit_app_md",
    "collapsed": false,
    "resultHeight": 47
   },
   "source": "#### Finally, the code below will deploy the chat application directly in Snowflake."
  },
  {
   "cell_type": "code",
   "id": "0fff792b-7360-4663-82c6-7b01ab5eeb1e",
   "metadata": {
    "language": "sql",
    "name": "deploy_streamlit_app_sql",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "-- channge the team name!\nSET team_name = 'SUPERSTARS';\nSET app_name = $team_name || '_' || 'MI_EMS_PROTOCAL_ASSISTANT';\nSET app_title = $team_name || '-' || 'Michigan EMS Protocol Assistant';\n\n-- note, this is the older streamlit create syntax\nCREATE OR REPLACE STREAMLIT IDENTIFIER($app_name)\n    ROOT_LOCATION = {{st_stage}}\n    MAIN_FILE = '/streamlit_main.py'\n    QUERY_WAREHOUSE = {{query_warehouse}}\n    TITLE = $app_title;",
   "execution_count": null
  }
 ]
}